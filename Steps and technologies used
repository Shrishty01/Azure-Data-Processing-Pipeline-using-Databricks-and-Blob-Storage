✅ Steps Performed
1️⃣ Azure Setup

Created a Storage Account to upload CSV files.

Created a Databricks Workspace for data processing.

Created a SQL Database to store cleaned data (optional depending on usage).

Ensured proper access by configuring keys or SAS tokens.

2️⃣ Data Upload

Uploaded multiple CSV files containing transactional data (e.g., sales orders) to Blob Storage containers.

Verified file format and structure before processing.

3️⃣ Databricks Data Processing

Mounted Blob Storage or configured access using wasbs:// protocol.

Read CSV files into Spark DataFrame using .read.format("csv").

Cleaned the data by:

Removing unwanted characters (e.g., $ symbols).

Casting numeric fields to proper data types.

Formatting date columns to yyyy-MM-dd.

Merged columns where necessary.

Dropped rows with missing values, invalid data, or duplicates.

4️⃣ Data Output

Wrote the cleaned dataset back to storage or to an Azure SQL Database for further analysis.

Ensured schema consistency and error handling during write operations.

5️⃣ Resource Management

Enabled Auto-Termination on Databricks clusters to save costs.

Reviewed and deleted unused Azure resources such as Storage Account, SQL Database, or Data Factory pipelines.

Checked billing to ensure that the subscription was not consuming credits unnecessarily.

⚙ Technologies Used

Azure Storage Account (Blob containers)

Azure Databricks

PySpark (DataFrame API)

Azure SQL Database

Azure Portal (for setup and management)
