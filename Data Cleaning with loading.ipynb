{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd63fb2-edf1-49bc-846a-cae289f806c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\n|SALESORDERID|SALESORDERITEM|PRODUCTID|NOTEID|CURRENCY|GROSSAMOUNT|NETAMOUNT|TAXAMOUNT|ITEMATPSTATUS|OPITEMPOS|QUANTITY|QUANTITYUNIT|DELIVERYDATE|\n+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\n|   500000000|            10|  MB-1034|      |     USD|  $2,499.00|$2,186.63|  $312.38|            I|     NULL|       4|          EA|    20180311|\n|   500000000|            20|  CB-1161|      |     USD|    $399.00|  $349.13|   $49.88|            I|     NULL|       9|          EA|    20180311|\n|   500000001|            10|  HB-1175|      |     USD|    $899.00|  $786.63|  $112.38|            I|     NULL|       2|          EA|    20180228|\n|   500000001|            20|  RC-1056|      |     USD|  $2,499.00|$2,186.63|  $312.38|            I|     NULL|       2|          EA|    20180228|\n|   500000001|            30|  CC-1021|      |     USD|  $1,144.00|$1,001.00|  $143.00|            I|     NULL|       3|          EA|    20180228|\n+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "storage_account = \"csvstoragedemo01\"\n",
    "container = \"raw-csv\"\n",
    "sas_token = \"sp=r&st=2025-09-13T13:26:24Z&se=2025-09-13T21:41:24Z&spr=https&sv=2024-11-04&sr=c&sig=dZinMuZrBMPpAivg%2BO3w9whkokYy4zc%2BjAUXUOwgTOU%3D\"  \n",
    "\n",
    "# Configure Databricks to access blob\n",
    "spark.conf.set(f\"fs.azure.sas.{container}.{storage_account}.blob.core.windows.net\", sas_token)\n",
    "\n",
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/SalesOrderItems.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc31ca4-2779-4ace-9134-0cf7e92e22c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SalesOrderItems.csv\n"
     ]
    }
   ],
   "source": [
    "# List all files and folders in the container\n",
    "files = dbutils.fs.ls(csv_path)\n",
    "\n",
    "# Display the files\n",
    "for file in files:\n",
    "    print(file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0828c0c-24b5-4ca0-a764-69490a9cc085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\n|SALESORDERID|SALESORDERITEM|PRODUCTID|NOTEID|CURRENCY|GROSSAMOUNT|NETAMOUNT|TAXAMOUNT|ITEMATPSTATUS|OPITEMPOS|QUANTITY|QUANTITYUNIT|DELIVERYDATE|\n+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\n|   500000000|            10|  MB-1034|  NULL|     USD|     2499.0|  2186.63|   312.38|            I|     NULL|       4|          EA|  2018-03-11|\n|   500000000|            20|  CB-1161|  NULL|     USD|      399.0|   349.13|    49.88|            I|     NULL|       9|          EA|  2018-03-11|\n|   500000001|            10|  HB-1175|  NULL|     USD|      899.0|   786.63|   112.38|            I|     NULL|       2|          EA|  2018-02-28|\n|   500000001|            20|  RC-1056|  NULL|     USD|     2499.0|  2186.63|   312.38|            I|     NULL|       2|          EA|  2018-02-28|\n|   500000001|            30|  CC-1021|  NULL|     USD|     1144.0|   1001.0|    143.0|            I|     NULL|       3|          EA|  2018-02-28|\n+------------+--------------+---------+------+--------+-----------+---------+---------+-------------+---------+--------+------------+------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"SALESORDERID\"),\n",
    "        col(\"SALESORDERITEM\").cast(\"int\").alias(\"SALESORDERITEM\"),\n",
    "        col(\"PRODUCTID\"),\n",
    "        col(\"NOTEID\").cast(\"int\").alias(\"NOTEID\"),\n",
    "        col(\"CURRENCY\"),\n",
    "        regexp_replace(regexp_replace(col(\"GROSSAMOUNT\"), \"[$]\", \"\"), \",\", \"\").cast(\"double\").alias(\"GROSSAMOUNT\"),\n",
    "        regexp_replace(regexp_replace(col(\"NETAMOUNT\"), \"[$]\", \"\"), \",\", \"\").cast(\"double\").alias(\"NETAMOUNT\"),\n",
    "        regexp_replace(col(\"TAXAMOUNT\"), \"[$]\", \"\").cast(\"double\").alias(\"TAXAMOUNT\"),\n",
    "        col(\"ITEMATPSTATUS\"),\n",
    "        col(\"OPITEMPOS\").cast(\"int\").alias(\"OPITEMPOS\"),\n",
    "        col(\"QUANTITY\").cast(\"int\").alias(\"QUANTITY\"),\n",
    "        col(\"QUANTITYUNIT\"),\n",
    "        date_format(to_date(col(\"DELIVERYDATE\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"DELIVERYDATE\")\n",
    "        \n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4c67a0-97da-4c2a-b0f4-dc2bfbd00fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+---------+---------+---------+-----------+----------------+------+---------+--------+--------+-----------+----------+----------+---------------+-------------+--------------+\n|SALESORDERID|CREATEDBY|CREATEDAT|CHANGEDBY|CHANGEDAT|FISCVARIANT|FISCALYEARPERIOD|NOTEID|PARTNERID|SALESORG|CURRENCY|GROSSAMOUNT| NETAMOUNT| TAXAMOUNT|LIFECYCLESTATUS|BILLINGSTATUS|DELIVERYSTATUS|\n+------------+---------+---------+---------+---------+-----------+----------------+------+---------+--------+--------+-----------+----------+----------+---------------+-------------+--------------+\n|   500000000|        4| 20180111|        4| 20180116|         K4|         2018001|  NULL|100000022|     APJ|     USD| $13,587.00|$11,888.63| $1,698.38|              C|            C|             C|\n|   500000001|        2| 20180112|        2| 20180115|         K4|         2018001|  NULL|100000026|    EMEA|     USD| $12,622.00|$11,044.25| $1,577.75|              C|            C|             C|\n|   500000002|        5| 20180115|        5| 20180120|         K4|         2018001|  NULL|100000018|     APJ|     USD| $45,655.00|$39,948.13| $5,706.88|              C|            C|             C|\n|   500000003|        3| 20180115|        3| 20180120|         K4|         2018001|  NULL|100000009|    EMEA|     USD|$101,786.00|$89,062.75|$12,723.25|              C|            C|             C|\n|   500000004|        8| 20180116|        8| 20180117|         K4|         2018001|  NULL|100000025|    EMEA|     USD| $71,684.00|$62,723.50| $8,960.50|              C|            C|             C|\n+------------+---------+---------+---------+---------+-----------+----------------+------+---------+--------+--------+-----------+----------+----------+---------------+-------------+--------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "container_name = 'raw-csv'\n",
    "storage_account_name = 'csvstoragedemo01'\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", \"sp=rl&st=2025-09-05T08:21:14Z&se=2025-09-12T16:36:14Z&spr=https&sv=2024-11-04&sr=c&sig=8WdP5niQzFyAhIf2dtLki5tB%2FrP%2BCDqSbUkJv7SGols%3D\")\n",
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/SalesOrders.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43985c6b-a88b-4228-9163-56a6ea51aeee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+---------+----------+-----------+----------------+------+---------+--------+--------+-----------+---------+---------+-------------+---------------+--------------+\n|SALESORDERID|CREATEDBY|CREATEDATE|CHANGEDBY|CHANGEDATE|FISCVARIANT|FISCALYEARPERIOD|NOTEID|PARTNERID|SALESORG|CURRENCY|GROSSAMOUNT|NETAMOUNT|TAXAMOUNT|BILLINGSTATUS|LIFECYCLESTATUS|DELIVERYSTATUS|\n+------------+---------+----------+---------+----------+-----------+----------------+------+---------+--------+--------+-----------+---------+---------+-------------+---------------+--------------+\n|   500000000|        4|2018-01-11|        4|2018-01-16|         K4|            NULL|  NULL|100000022|     APJ|     USD|    13587.0| 11888.63|     NULL|            C|              C|             C|\n|   500000001|        2|2018-01-12|        2|2018-01-15|         K4|            NULL|  NULL|100000026|    EMEA|     USD|    12622.0| 11044.25|     NULL|            C|              C|             C|\n|   500000002|        5|2018-01-15|        5|2018-01-20|         K4|            NULL|  NULL|100000018|     APJ|     USD|    45655.0| 39948.13|     NULL|            C|              C|             C|\n|   500000003|        3|2018-01-15|        3|2018-01-20|         K4|            NULL|  NULL|100000009|    EMEA|     USD|   101786.0| 89062.75|     NULL|            C|              C|             C|\n|   500000004|        8|2018-01-16|        8|2018-01-17|         K4|            NULL|  NULL|100000025|    EMEA|     USD|    71684.0|  62723.5|     NULL|            C|              C|             C|\n+------------+---------+----------+---------+----------+-----------+----------------+------+---------+--------+--------+-----------+---------+---------+-------------+---------------+--------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"SALESORDERID\").cast(\"string\").alias(\"SALESORDERID\"),\n",
    "        col(\"CREATEDBY\"),\n",
    "        date_format(to_date(col(\"CREATEDAT\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CREATEDATE\"),\n",
    "        col(\"CHANGEDBY\"),\n",
    "        date_format(to_date(col(\"CHANGEDAT\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CHANGEDATE\"),\n",
    "        col(\"FISCVARIANT\"),\n",
    "        date_format(to_date(col(\"FISCALYEARPERIOD\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"FISCALYEARPERIOD\"),\n",
    "        col(\"NOTEID\"),\n",
    "        col(\"PARTNERID\").cast(\"string\").alias(\"PARTNERID\"),\n",
    "        col(\"SALESORG\"),\n",
    "        col(\"CURRENCY\"),\n",
    "        regexp_replace(regexp_replace(col(\"GROSSAMOUNT\"), \"[$]\", \"\"), \",\", \"\").cast(\"double\").alias(\"GROSSAMOUNT\"),\n",
    "        regexp_replace(regexp_replace(col(\"NETAMOUNT\"), \"[$]\", \"\"), \",\", \"\").cast(\"double\").alias(\"NETAMOUNT\"),\n",
    "        regexp_replace(col(\"TAXAMOUNT\"), \"[$]\", \"\").cast(\"double\").alias(\"TAXAMOUNT\"),\n",
    "        col(\"BILLINGSTATUS\"),\n",
    "        col(\"LIFECYCLESTATUS\"),\n",
    "        col(\"DELIVERYSTATUS\")\n",
    "        \n",
    "        \n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3422821d-28d2-4c50-ab5b-d0a2c6f60180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------+---------+---------+---------+---------+------------------+-------------+------------+-------------+----------+--------+-------+-----+-----+------+-------------+-------------+\n|PRODUCTID|TYPECODE|PRODCATEGORYID|CREATEDBY|CREATEDAT|CHANGEDBY|CHANGEDAT|SUPPLIER_PARTNERID|TAXTARIFFCODE|QUANTITYUNIT|WEIGHTMEASURE|WEIGHTUNIT|CURRENCY|  PRICE|WIDTH|DEPTH|HEIGHT|DIMENSIONUNIT|PRODUCTPICURL|\n+---------+--------+--------------+---------+---------+---------+---------+------------------+-------------+------------+-------------+----------+--------+-------+-----+-----+------+-------------+-------------+\n|  RO-1001|      PR|            RO|        9| 20181003|        9| 20181003|         100000000|            1|          EA|          7.7|        KG|     USD|$525.00| NULL| NULL|  NULL|         NULL|         NULL|\n|  RO-1002|      PR|            RO|        9| 20181003|        9| 20181003|         100000001|            1|          EA|          8.0|        KG|     USD|$689.00| NULL| NULL|  NULL|         NULL|         NULL|\n|  RO-1003|      PR|            RO|       12| 20181003|       12| 20181003|         100000002|            1|          EA|          9.1|        KG|     USD|$721.00| NULL| NULL|  NULL|         NULL|         NULL|\n|  BX-1011|      PR|            BX|        9| 20181003|        9| 20181003|         100000003|            1|          EA|         11.1|        KG|     USD|$249.00| NULL| NULL|  NULL|         NULL|         NULL|\n|  BX-1012|      PR|            BX|        6| 20181003|        6| 20181003|         100000004|            1|          EA|         12.0|        KG|     USD|$399.00| NULL| NULL|  NULL|         NULL|         NULL|\n+---------+--------+--------------+---------+---------+---------+---------+------------------+-------------+------------+-------------+----------+--------+-------+-----+-----+------+-------------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "container_name = 'raw-csv'\n",
    "storage_account_name = 'csvstoragedemo01'\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", \"sp=rl&st=2025-09-05T08:21:14Z&se=2025-09-12T16:36:14Z&spr=https&sv=2024-11-04&sr=c&sig=8WdP5niQzFyAhIf2dtLki5tB%2FrP%2BCDqSbUkJv7SGols%3D\")\n",
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Products.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e4ec8b-ecc9-440c-ab84-8ebb2182c105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------+---------+----------+---------+----------+------------------+-------------+------------+-------------+----------+--------+-----+-----+-----+------+-------------+-------------+\n|PRODUCTID|TYPECODE|PRODCATEGORYID|CREATEDBY|CREATEDATE|CHANGEDBY|CHANGEDATE|SUPPLIER_PARTNERID|TAXTARIFFCODE|QUANTITYUNIT|WEIGHTMEASURE|WEIGHTUNIT|CURRENCY|PRICE|WIDTH|DEPTH|HEIGHT|DIMENSIONUNIT|PRODUCTPICURL|\n+---------+--------+--------------+---------+----------+---------+----------+------------------+-------------+------------+-------------+----------+--------+-----+-----+-----+------+-------------+-------------+\n|  RO-1001|      PR|            RO|        9|2018-10-03|        9|2018-10-03|         100000000|            1|          EA|          7.7|        KG|     USD|525.0| NULL| NULL|  NULL|         NULL|         NULL|\n|  RO-1002|      PR|            RO|        9|2018-10-03|        9|2018-10-03|         100000001|            1|          EA|          8.0|        KG|     USD|689.0| NULL| NULL|  NULL|         NULL|         NULL|\n|  RO-1003|      PR|            RO|       12|2018-10-03|       12|2018-10-03|         100000002|            1|          EA|          9.1|        KG|     USD|721.0| NULL| NULL|  NULL|         NULL|         NULL|\n|  BX-1011|      PR|            BX|        9|2018-10-03|        9|2018-10-03|         100000003|            1|          EA|         11.1|        KG|     USD|249.0| NULL| NULL|  NULL|         NULL|         NULL|\n|  BX-1012|      PR|            BX|        6|2018-10-03|        6|2018-10-03|         100000004|            1|          EA|         12.0|        KG|     USD|399.0| NULL| NULL|  NULL|         NULL|         NULL|\n+---------+--------+--------------+---------+----------+---------+----------+------------------+-------------+------------+-------------+----------+--------+-----+-----+-----+------+-------------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"PRODUCTID\"),\n",
    "        col(\"TYPECODE\"),\n",
    "        col(\"PRODCATEGORYID\"),\n",
    "        col(\"CREATEDBY\"),\n",
    "        date_format(to_date(col(\"CREATEDAT\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CREATEDATE\"),\n",
    "        col(\"CHANGEDBY\"),\n",
    "        date_format(to_date(col(\"CHANGEDAT\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CHANGEDATE\"),\n",
    "        col(\"SUPPLIER_PARTNERID\").cast(\"int\").alias(\"SUPPLIER_PARTNERID\"),\n",
    "        col(\"TAXTARIFFCODE\"),\n",
    "        col(\"QUANTITYUNIT\"),\n",
    "        col(\"WEIGHTMEASURE\"),\n",
    "        col(\"WEIGHTUNIT\"),\n",
    "        col(\"CURRENCY\"),\n",
    "        regexp_replace(regexp_replace(col(\"PRICE\"), \"[$]\", \"\"), \",\", \"\").cast(\"double\").alias(\"PRICE\"),\n",
    "        col(\"WIDTH\").cast(\"double\").alias(\"WIDTH\"),\n",
    "        col(\"DEPTH\").cast(\"double\").alias(\"DEPTH\"),\n",
    "        col(\"HEIGHT\").cast(\"double\").alias(\"HEIGHT\"),\n",
    "        col(\"DIMENSIONUNIT\"),\n",
    "        col(\"PRODUCTPICURL\")        \n",
    "        \n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efdf42c8-eabc-48d1-aef0-f40c801c8815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+------------+----------+\n|PRODUCTID|LANGUAGE|     SHORT_DESCR|MEDIUM_DESCR|LONG_DESCR|\n+---------+--------+----------------+------------+----------+\n|  RO-1001|      EN|      Roady 1001|        NULL|      NULL|\n|  RO-1002|      EN|      Roady 1002|        NULL|      NULL|\n|  RO-1003|      EN|      Roady 1003|        NULL|      NULL|\n|  BX-1011|      EN|BMX Vintage 1011|        NULL|      NULL|\n|  BX-1012|      EN|   BMX Jump 1012|        NULL|      NULL|\n+---------+--------+----------------+------------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.conf.set(\"fs.azure.account.key.{storage_account}.blob.core.windows.net\", \"sp=rl&st=2025-09-05T08:21:14Z&se=2025-09-12T16:36:14Z&spr=https&sv=2024-11-04&sr=c&sig=8WdP5niQzFyAhIf2dtLki5tB%2FrP%2BCDqSbUkJv7SGols%3D\")\n",
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/ProductTexts.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe4aa584-7ae4-4330-ab37-b8549819740e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n|PRODCATEGORYID|CREATEDBY|CREATEDAT|\n+--------------+---------+---------+\n|            RO|       12| 20181003|\n|            BX|        4| 20181003|\n|            CC|        7| 20181003|\n|            MB|       11| 20181003|\n|            RC|        9| 20181003|\n+--------------+---------+---------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/ProductCategories.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bc892c-b671-4046-8601-fe832dbf5efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----------+\n|PRODCATEGORYID|CREATEDBY|CREATEDATE|\n+--------------+---------+----------+\n|            RO|       12|2018-10-03|\n|            BX|        4|2018-10-03|\n|            CC|        7|2018-10-03|\n|            MB|       11|2018-10-03|\n|            RC|        9|2018-10-03|\n+--------------+---------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"PRODCATEGORYID\"),\n",
    "        col(\"CREATEDBY\"),\n",
    "        date_format(to_date(col(\"CREATEDAT\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CREATEDATE\"),       \n",
    "        \n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18adce8-28b6-4956-996e-038426b2ed36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----------------+------------+----------+\n|PRODCATEGORYID|LANGUAGE|     SHORT_DESCR|MEDIUM_DESCR|LONG_DESCR|\n+--------------+--------+----------------+------------+----------+\n|            RO|      EN|       Road Bike|        NULL|      NULL|\n|            BX|      EN|             BMX|        NULL|      NULL|\n|            CC|      EN|Cyclo-cross Bike|        NULL|      NULL|\n|            MB|      EN|   Mountain Bike|        NULL|      NULL|\n|            RC|      EN|     Racing Bike|        NULL|      NULL|\n+--------------+--------+----------------+------------+----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/ProductCategoryText.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa53fbef-c6d2-41b2-8ea2-c1845389958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---------+-------------+---+--------+--------------+--------------------+---------+----------+------------------+----------------+----+----+----+----+----+----+\n|EMPLOYEEID|NAME_FIRST|NAME_MIDDLE|NAME_LAST|NAME_INITIALS|SEX|LANGUAGE|   PHONENUMBER|        EMAILADDRESS|LOGINNAME| ADDRESSID|VALIDITY_STARTDATE|VALIDITY_ENDDATE|_c13|_c14|_c15|_c16|_c17|_c18|\n+----------+----------+-----------+---------+-------------+---+--------+--------------+--------------------+---------+----------+------------------+----------------+----+----+----+----+----+----+\n|         1|   Derrick|          L|   Magill|         NULL|  M|       E|  630-374-0306|derrick.magill@it...| derrickm|1000000001|          20000101|        99991231|NULL|NULL|NULL|NULL|NULL|NULL|\n|         2|   Philipp|          T|    Egger|         NULL|  M|       E|09603 61 24 64|philipp.egger@ite...| philippm|1000000002|          20000101|        99991231|NULL|NULL|NULL|NULL|NULL|NULL|\n|         3|     Ellis|          K|Robertson|         NULL|  M|       E| 070 8691 2288|ellis.robertson@i...|   ellism|1000000003|          20000101|        99991231|NULL|NULL|NULL|NULL|NULL|NULL|\n|         4|   William|          M|   Mussen|         NULL|  M|       E|   026734 4556|william.mussen@it...| williamm|1000000004|          20000101|        99991231|NULL|NULL|NULL|NULL|NULL|NULL|\n|         5|     Javas|       NULL|    Hegde|         NULL|  M|       E|   02224135120|javas.hegde@itelo...|   javasm|1000000005|          20000101|        99991231|NULL|NULL|NULL|NULL|NULL|NULL|\n+----------+----------+-----------+---------+-------------+---+--------+--------------+--------------------+---------+----------+------------------+----------------+----+----+----+----+----+----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Employees.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b435f6-ae13-4388-8645-f653a59b9b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---------+-----------------+-------------+---+--------+-----------+--------------------+---------+----------+------------------+----------------+\n|EMPLOYEEID|NAME_FIRST|NAME_MIDDLE|NAME_LAST|    EMPLOYEE_NAME|NAME_INITIALS|SEX|LANGUAGE|PHONENUMBER|        EMAILADDRESS|LOGINNAME| ADDRESSID|VALIDITY_STARTDATE|VALIDITY_ENDDATE|\n+----------+----------+-----------+---------+-----------------+-------------+---+--------+-----------+--------------------+---------+----------+------------------+----------------+\n|         1|   Derrick|          L|   Magill| Derrick L Magill|         NULL|  M|       E| 6303740306|derrick.magill@it...| derrickm|1000000001|        2000-01-01|      9999-12-31|\n|         2|   Philipp|          T|    Egger|  Philipp T Egger|         NULL|  M|       E|09603612464|philipp.egger@ite...| philippm|1000000002|        2000-01-01|      9999-12-31|\n|         3|     Ellis|          K|Robertson|Ellis K Robertson|         NULL|  M|       E|07086912288|ellis.robertson@i...|   ellism|1000000003|        2000-01-01|      9999-12-31|\n|         4|   William|          M|   Mussen| William M Mussen|         NULL|  M|       E| 0267344556|william.mussen@it...| williamm|1000000004|        2000-01-01|      9999-12-31|\n|         5|     Javas|       NULL|    Hegde|             NULL|         NULL|  M|       E|02224135120|javas.hegde@itelo...|   javasm|1000000005|        2000-01-01|      9999-12-31|\n+----------+----------+-----------+---------+-----------------+-------------+---+--------+-----------+--------------------+---------+----------+------------------+----------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format, concat, lit\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"EMPLOYEEID\"),\n",
    "        col(\"NAME_FIRST\"),\n",
    "        col(\"NAME_MIDDLE\"),\n",
    "        col(\"NAME_LAST\"),\n",
    "        concat(col(\"NAME_FIRST\"),lit(\" \"), col(\"NAME_MIDDLE\"),lit(\" \"), col(\"NAME_LAST\")).alias(\"EMPLOYEE_NAME\"),\n",
    "        col(\"NAME_INITIALS\"),\n",
    "        col(\"SEX\"),\n",
    "        col(\"LANGUAGE\"),\n",
    "        regexp_replace(regexp_replace(col(\"PHONENUMBER\"),\"[-]\",\"\"),\" \",\"\").alias(\"PHONENUMBER\"),\n",
    "        col(\"EMAILADDRESS\"),\n",
    "        col(\"LOGINNAME\"),\n",
    "        col(\"ADDRESSID\"),\n",
    "        date_format(to_date(col(\"VALIDITY_STARTDATE\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"VALIDITY_STARTDATE\"),\n",
    "        date_format(to_date(col(\"VALIDITY_ENDDATE\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"VALIDITY_ENDDATE\")\n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.drop(col(\"_c13\"), col(\"_c14\"), col(\"_c15\"), col(\"_c16\"), col(\"_c17\"), col(\"_c18\"))\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbe3706-1611-4091-9694-8666546e462f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+---------+---------+---------+--------+\n|PARTNERID|PARTNERROLE|        EMAILADDRESS|PHONENUMBER|FAXNUMBER|          WEBADDRESS| ADDRESSID|     COMPANYNAME|LEGALFORM|CREATEDBY|CREATEDAT|CHANGEDBY|CHANGEDAT|CURRENCY|\n+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+---------+---------+---------+--------+\n|100000000|          2|maria.brown@all4b...|  622734567|     NULL|http://www.all4bi...|1000000034|   All For Bikes|     Inc.|       10| 20181003|       10| 20181003|     USD|\n|100000001|          2|bob.buyer@amazebi...|    3088530|     NULL|http://www.amazeb...|1000000035| Amaze Bikes Inc|     Inc.|       13| 20181003|       13| 20181003|     USD|\n|100000002|          2|victor.sanchez@ar...| 3023352668|     NULL|http://www.arenas...|1000000036|Arena Sports Inc|     Inc.|       14| 20181003|       14| 20181003|     USD|\n|100000003|          2|franklin.jones@at...|  511403266|     NULL|http://www.atlant...|1000000037|Atlanta Corp Inc|     Inc.|       10| 20181003|       10| 20181003|     USD|\n|100000004|          2|robert_brown@bike...| 2244668800|     NULL|http://www.bikewo...|1000000038|  Bike World Inc|     Inc.|        4| 20181003|        4| 20181003|     USD|\n+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+---------+---------+---------+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/BusinessPartners.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16421a3-21a0-4f32-a318-c4cb89afda21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+----------+---------+----------+--------+\n|PARTNERID|PARTNERROLE|        EMAILADDRESS|PHONENUMBER|FAXNUMBER|          WEBADDRESS| ADDRESSID|     COMPANYNAME|LEGALFORM|CREATEDBY|CREATEDATE|CHANGEDBY|CHANGEDATE|CURRENCY|\n+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+----------+---------+----------+--------+\n|100000000|          2|maria.brown@all4b...|  622734567|     NULL|http://www.all4bi...|1000000034|   All For Bikes|     Inc.|       10|2018-10-03|       10|2018-10-03|     USD|\n|100000001|          2|bob.buyer@amazebi...|    3088530|     NULL|http://www.amazeb...|1000000035| Amaze Bikes Inc|     Inc.|       13|2018-10-03|       13|2018-10-03|     USD|\n|100000002|          2|victor.sanchez@ar...| 3023352668|     NULL|http://www.arenas...|1000000036|Arena Sports Inc|     Inc.|       14|2018-10-03|       14|2018-10-03|     USD|\n|100000003|          2|franklin.jones@at...|  511403266|     NULL|http://www.atlant...|1000000037|Atlanta Corp Inc|     Inc.|       10|2018-10-03|       10|2018-10-03|     USD|\n|100000004|          2|robert_brown@bike...| 2244668800|     NULL|http://www.bikewo...|1000000038|  Bike World Inc|     Inc.|        4|2018-10-03|        4|2018-10-03|     USD|\n+---------+-----------+--------------------+-----------+---------+--------------------+----------+----------------+---------+---------+----------+---------+----------+--------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"PARTNERID\").cast(\"string\").alias(\"PARTNERID\"),\n",
    "        col(\"PARTNERROLE\"),\n",
    "        col(\"EMAILADDRESS\"),\n",
    "        col(\"PHONENUMBER\"),\n",
    "        col(\"FAXNUMBER\"),\n",
    "        col(\"WEBADDRESS\"),\n",
    "        col(\"ADDRESSID\").cast(\"string\").alias(\"ADDRESSID\"),\n",
    "        col(\"COMPANYNAME\"),\n",
    "        col(\"LEGALFORM\"),\n",
    "        col(\"CREATEDBY\"),\n",
    "        date_format(to_date(col(\"CREATEDAT\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CREATEDATE\"),\n",
    "        col(\"CHANGEDBY\"),\n",
    "        date_format(to_date(col(\"CHANGEDAT\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"CHANGEDATE\"),\n",
    "        col(\"CURRENCY\")        \n",
    "        \n",
    ")\n",
    ")\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17460ba9-5d90-41b5-a893-22df7ade266c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------------+--------+-------+------+-----------+------------------+----------------+---------+-----------+\n| ADDRESSID|      CITY|POSTALCODE|          STREET|BUILDING|COUNTRY|REGION|ADDRESSTYPE|VALIDITY_STARTDATE|VALIDITY_ENDDATE| LATITUDE|  LONGITUDE|\n+----------+----------+----------+----------------+--------+-------+------+-----------+------------------+----------------+---------+-----------+\n|1000000034|West Nyack|     10994|   Settlers Lane|    5027|     US|  AMER|          2|          20000101|        99991231|41.100057| -73.973562|\n|1000000035| Fair Oaks|     95628|Woodland Terrace|    4467|     US|  AMER|          2|          20000101|        99991231|38.638355|-121.286683|\n|1000000036|      Dunn|     28334|   Layman Avenue|    2250|     US|  AMER|          2|          20000101|        99991231|35.312013|  -78.60995|\n|1000000037|   Chicago|     60605|University Drive|    4697|     US|  AMER|          2|          20000101|        99991231|41.874591| -87.627303|\n|1000000038|     Ocala|     34471|  Bagwell Avenue|    1565|     US|  AMER|          2|          20000101|        99991231|29.183977| -82.118413|\n+----------+----------+----------+----------------+--------+-------+------+-----------+------------------+----------------+---------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "csv_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/Addresses.csv\"\n",
    "\n",
    "# Read CSV\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb14eca-fd2b-4b8a-9411-e34aa2f72f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------------+--------+-------+------+--------------------+-----------+------------------+----------------+---------+-----------+\n| ADDRESSID|      CITY|POSTALCODE|          STREET|BUILDING|COUNTRY|REGION|             ADDRESS|ADDRESSTYPE|VALIDITY_STARTDATE|VALIDITY_ENDDATE| LATITUDE|  LONGITUDE|\n+----------+----------+----------+----------------+--------+-------+------+--------------------+-----------+------------------+----------------+---------+-----------+\n|1000000034|West Nyack|     10994|   Settlers Lane|    5027|     US|  AMER|5027 Settlers Lan...|          2|        2000-01-01|      9999-12-31|41.100057| -73.973562|\n|1000000035| Fair Oaks|     95628|Woodland Terrace|    4467|     US|  AMER|4467 Woodland Ter...|          2|        2000-01-01|      9999-12-31|38.638355|-121.286683|\n|1000000036|      Dunn|     28334|   Layman Avenue|    2250|     US|  AMER|2250 Layman Avenu...|          2|        2000-01-01|      9999-12-31|35.312013|  -78.60995|\n|1000000037|   Chicago|     60605|University Drive|    4697|     US|  AMER|4697 University D...|          2|        2000-01-01|      9999-12-31|41.874591| -87.627303|\n|1000000038|     Ocala|     34471|  Bagwell Avenue|    1565|     US|  AMER|1565 Bagwell Aven...|          2|        2000-01-01|      9999-12-31|29.183977| -82.118413|\n+----------+----------+----------+----------------+--------+-------+------+--------------------+-----------+------------------+----------------+---------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, to_date, date_format, concat, lit\n",
    "\n",
    "df_clean = (\n",
    "    df.select(\n",
    "        col(\"ADDRESSID\"),\n",
    "        col(\"CITY\"),\n",
    "        col(\"POSTALCODE\"),\n",
    "        col(\"STREET\"),\n",
    "        col(\"BUILDING\"),\n",
    "        col(\"COUNTRY\"),\n",
    "        col(\"REGION\"),\n",
    "        concat(col(\"BUILDING\"),lit(\" \"), col(\"STREET\"),lit(\" \"), col(\"CITY\"),lit(\" \"), col(\"POSTALCODE\")).alias(\"ADDRESS\"),\n",
    "        col(\"ADDRESSTYPE\"),\n",
    "        date_format(to_date(col(\"VALIDITY_STARTDATE\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"VALIDITY_STARTDATE\"),\n",
    "        date_format(to_date(col(\"VALIDITY_ENDDATE\").cast(\"string\"), \"yyyyMMdd\"), \"yyyy-MM-dd\").alias(\"VALIDITY_ENDDATE\"),\n",
    "        col(\"LATITUDE\"),\n",
    "        col(\"LONGITUDE\")\n",
    "       \n",
    ")\n",
    ")\n",
    "\n",
    "\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c1568d0-7b11-41ae-a54c-ae532bd7c63b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:sqlserver://ssmsserverbi.database.windows.net:1433;database=PowerbiDB;authentication=ActiveDirectoryPassword\"\n",
    "\n",
    "connection_props = {\n",
    "  \"user\": \"yourAADUser@yourdomain.com\",\n",
    "  \"password\": \"<your AAD password>\",\n",
    "  \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BusinessData",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}